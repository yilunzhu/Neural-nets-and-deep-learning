{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T22:02:22.434845Z",
     "start_time": "2019-12-10T22:02:22.423904Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import applications\n",
    "from keras import backend\n",
    "from keras.preprocessing.text import Tokenizer, one_hot\n",
    "from keras.preprocessing import sequence\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Dropout, Embedding, LSTM, Bidirectional, Conv1D, MaxPooling1D, Activation\n",
    "from keras.models import Model, Sequential\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T20:43:00.487006Z",
     "start_time": "2019-12-09T20:42:59.841732Z"
    }
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "\n",
    "input_shape = x_train.shape\n",
    "pixel_len = input_shape[1]\n",
    "\n",
    "x_train = x_train.reshape(len(x_train), pixel_len, pixel_len, 1)\n",
    "x_test = x_test.reshape(len(x_test), pixel_len, pixel_len, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T20:43:33.631349Z",
     "start_time": "2019-12-09T20:43:33.624365Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T06:48:20.971907Z",
     "start_time": "2019-12-09T06:48:20.527081Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T06:51:12.257190Z",
     "start_time": "2019-12-09T06:51:12.112579Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 14, 14, 1)         289       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 1)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 7, 7, 1)           10        \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 14, 14, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 14, 14, 32)        320       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2 (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 28, 28, 1)         289       \n",
      "=================================================================\n",
      "Total params: 1,228\n",
      "Trainable params: 1,228\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_img = Input(shape=(pixel_len, pixel_len, 1))\n",
    "\n",
    "# encoder\n",
    "model = Conv2D(32, (3, 3), padding='same', activation='relu')(input_img)\n",
    "model = MaxPooling2D((2, 2), padding='same')(model)\n",
    "model = Conv2D(1,(3, 3), padding='same', activation='relu')(model)\n",
    "model = MaxPooling2D((2, 2), padding='same')(model)\n",
    "\n",
    "# decoder\n",
    "model = Conv2D(1,(3, 3), padding='same', activation='relu')(model)\n",
    "model = UpSampling2D((2, 2))(model)\n",
    "model = Conv2D(32, (3, 3), padding='same', activation='relu')(model)\n",
    "model = UpSampling2D((2, 2))(model)\n",
    "model = Conv2D(1, (3, 3), padding='same', activation='sigmoid')(model)\n",
    "\n",
    "autoencoder = Model(input_img, model)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T07:10:11.538615Z",
     "start_time": "2019-12-09T06:53:59.506284Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 97s 2ms/step - loss: 0.0248 - val_loss: 0.0230\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 97s 2ms/step - loss: 0.0223 - val_loss: 0.0209\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 97s 2ms/step - loss: 0.0203 - val_loss: 0.0191\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 97s 2ms/step - loss: 0.0188 - val_loss: 0.0178\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 97s 2ms/step - loss: 0.0177 - val_loss: 0.0169\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 97s 2ms/step - loss: 0.0169 - val_loss: 0.0163\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 97s 2ms/step - loss: 0.0164 - val_loss: 0.0160\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 99s 2ms/step - loss: 0.0160 - val_loss: 0.0155\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 97s 2ms/step - loss: 0.0157 - val_loss: 0.0153\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 96s 2ms/step - loss: 0.0155 - val_loss: 0.0151\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1cbb8de9ba8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(x_train, x_train, epochs=10, batch_size=128, shuffle=True, validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T07:12:32.327181Z",
     "start_time": "2019-12-09T07:12:28.748752Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_img = autoencoder.predict(x_test)\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(pred_img[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Image classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Deep CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CNN model includes two convolutional transformations that uses (1) 128 filters with a 3x3 kernel and (2) 32 filters with 3x3 filters with a 3x3 kernel, and one pooling layer: max pooling with a 2x2 window. Then the model flattens the vectors and goes through two dense layers to get the output, which is the integer that maps to each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T07:43:21.701017Z",
     "start_time": "2019-12-10T07:43:21.684063Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T07:22:13.201387Z",
     "start_time": "2019-12-09T07:22:12.980978Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_11 (Conv2D)           (None, 26, 26, 128)       1280      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 13, 13, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 13, 13, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 11, 11, 32)        36896     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3872)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               495744    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 535,210\n",
      "Trainable params: 535,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_model = Sequential()\n",
    "cnn_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', input_shape=(pixel_len, pixel_len, 1)))\n",
    "cnn_model.add(MaxPooling2D((2, 2)))\n",
    "cnn_model.add(Dropout(0.3))\n",
    "cnn_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "cnn_model.add(Flatten())\n",
    "cnn_model.add(Dense(128, activation='relu'))\n",
    "cnn_model.add(Dropout(0.5))\n",
    "cnn_model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T07:41:18.893145Z",
     "start_time": "2019-12-09T07:22:59.106127Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 98s 2ms/step - loss: 0.2799 - accuracy: 0.9140 - val_loss: 0.0582 - val_accuracy: 0.9806\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 94s 2ms/step - loss: 0.0910 - accuracy: 0.9726 - val_loss: 0.0424 - val_accuracy: 0.9849\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 94s 2ms/step - loss: 0.0657 - accuracy: 0.9803 - val_loss: 0.0319 - val_accuracy: 0.9900\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 153s 3ms/step - loss: 0.0524 - accuracy: 0.9847 - val_loss: 0.0309 - val_accuracy: 0.9887\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 106s 2ms/step - loss: 0.0450 - accuracy: 0.9864 - val_loss: 0.0285 - val_accuracy: 0.9908\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 93s 2ms/step - loss: 0.0390 - accuracy: 0.9879 - val_loss: 0.0256 - val_accuracy: 0.9912\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 94s 2ms/step - loss: 0.0335 - accuracy: 0.9894 - val_loss: 0.0323 - val_accuracy: 0.9891\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 94s 2ms/step - loss: 0.0312 - accuracy: 0.9905 - val_loss: 0.0259 - val_accuracy: 0.9918\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 167s 3ms/step - loss: 0.0273 - accuracy: 0.9911 - val_loss: 0.0239 - val_accuracy: 0.9926\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 105s 2ms/step - loss: 0.0243 - accuracy: 0.9921 - val_loss: 0.0274 - val_accuracy: 0.9914\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1cb873f68d0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model.fit(x_train, y_train,\n",
    "              batch_size=128,\n",
    "              epochs=10,\n",
    "              verbose=1,\n",
    "              validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T07:41:48.427970Z",
     "start_time": "2019-12-09T07:41:45.245177Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 3s 317us/step\n",
      "Test loss: 0.027442988229725643\n",
      "Test accuracy: 0.9914000034332275\n"
     ]
    }
   ],
   "source": [
    "cnn_eval_score = cnn_model.evaluate(x_test, y_test)\n",
    "print('Test loss:', cnn_eval_score[0])\n",
    "print('Test accuracy:', cnn_eval_score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T07:38:33.791390Z",
     "start_time": "2019-12-10T07:38:32.735212Z"
    }
   },
   "outputs": [],
   "source": [
    "transfer_x_train = np.tile(x_train, (1, 1, 1, 3))\n",
    "transfer_x_test = np.tile(x_test, (1, 1, 1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T07:38:37.761766Z",
     "start_time": "2019-12-10T07:38:34.775779Z"
    }
   },
   "outputs": [],
   "source": [
    "transfer_x_train = np.pad(transfer_x_train, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "transfer_x_test = np.pad(transfer_x_test, ((0,0),(2,2),(2,2),(0,0)), 'constant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T07:38:38.666387Z",
     "start_time": "2019-12-10T07:38:38.661398Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 32, 32, 3)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transfer_x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T07:42:21.804238Z",
     "start_time": "2019-12-10T07:42:03.533105Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Model)             (None, 1, 1, 2048)        23587712  \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 24,670,282\n",
      "Trainable params: 1,082,570\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transfer_model = Sequential()\n",
    "transfer_model.add(applications.resnet.ResNet50(weights = \"imagenet\", include_top=False, input_shape=(32, 32, 3)))\n",
    "\n",
    "# freeze the weights\n",
    "for layer in transfer_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Dense layer\n",
    "transfer_model.add(Flatten())\n",
    "transfer_model.add(Dense(512, activation='relu'))\n",
    "transfer_model.add(Dropout(0.35))\n",
    "transfer_model.add(Dense(64, activation='relu'))\n",
    "transfer_model.add(Dropout(0.35))\n",
    "transfer_model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "transfer_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "transfer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T08:23:56.663256Z",
     "start_time": "2019-12-10T07:43:28.644444Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 269s 4ms/step - loss: 1.0092 - accuracy: 0.6867 - val_loss: 2.8465 - val_accuracy: 0.1511\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 270s 4ms/step - loss: 0.6589 - accuracy: 0.7956 - val_loss: 3.4635 - val_accuracy: 0.1062\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 273s 5ms/step - loss: 0.5738 - accuracy: 0.8219 - val_loss: 3.1264 - val_accuracy: 0.1509\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 200s 3ms/step - loss: 0.5167 - accuracy: 0.8385 - val_loss: 2.8753 - val_accuracy: 0.1463\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 233s 4ms/step - loss: 0.4817 - accuracy: 0.8495 - val_loss: 2.9366 - val_accuracy: 0.1392\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 207s 3ms/step - loss: 0.4517 - accuracy: 0.8566 - val_loss: 2.8417 - val_accuracy: 0.1227\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 251s 4ms/step - loss: 0.4275 - accuracy: 0.8645 - val_loss: 3.2357 - val_accuracy: 0.1073\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 250s 4ms/step - loss: 0.4031 - accuracy: 0.8708 - val_loss: 2.8310 - val_accuracy: 0.1780\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 211s 4ms/step - loss: 0.3823 - accuracy: 0.8790 - val_loss: 3.0219 - val_accuracy: 0.1309\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 257s 4ms/step - loss: 0.3665 - accuracy: 0.8822 - val_loss: 3.8906 - val_accuracy: 0.1034\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2f7a4fc6a20>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transfer_model.fit(transfer_x_train, y_train,\n",
    "              batch_size=128,\n",
    "              epochs=10,\n",
    "              verbose=1,\n",
    "              validation_data=(transfer_x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T08:24:33.949514Z",
     "start_time": "2019-12-10T08:24:05.139581Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 29s 3ms/step\n",
      "Test loss: 3.8906137199401853\n",
      "Test accuracy: 0.10339999943971634\n"
     ]
    }
   ],
   "source": [
    "transfer_eval_score = transfer_model.evaluate(transfer_x_test, y_test)\n",
    "print('Test loss:', transfer_eval_score[0])\n",
    "print('Test accuracy:', transfer_eval_score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there anything wrong with my code? It's strange that the acc on the test set is pretty low..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm confused with how to evaluate the task. If I follow the description in the question, it is a binary classification task that whether the topic is discussed in the text. The other option is a topic ranking task, which assigns the probility to each topic and decides which topic(s) is(are) most possible discussed. \n",
    "\n",
    "I looked into the dataset and found that it is not always (actually a lot) the case that each row sum to 1. Thus I'm not sure how to evaluate the weights of each topic in the dataset. In this assignment, I change the labels to either 0 or 1, making it a multi-label binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T22:05:35.315418Z",
     "start_time": "2019-12-10T22:05:35.311430Z"
    }
   },
   "outputs": [],
   "source": [
    "max_seq_len = 64\n",
    "num_classes = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T21:57:39.438800Z",
     "start_time": "2019-12-10T21:57:39.189435Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('tweets_with_labels.csv', sep='\\t', header=[0])\n",
    "data_df = df['tweet']\n",
    "labels = df.iloc[:,2:] \n",
    "X_train, X_test, text_y_train, text_y_test = train_test_split(data_df, labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T22:05:09.230158Z",
     "start_time": "2019-12-10T22:05:09.211211Z"
    }
   },
   "outputs": [],
   "source": [
    "text_y_train = np.array(text_y_train)\n",
    "text_y_test = np.array(text_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T21:57:41.680490Z",
     "start_time": "2019-12-10T21:57:41.634607Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_y_train = np.where(text_y_train == 0, 0, 1)\n",
    "text_y_test = np.where(text_y_test == 0, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T21:57:44.846016Z",
     "start_time": "2019-12-10T21:57:43.133597Z"
    }
   },
   "outputs": [],
   "source": [
    "word_tokenizer = Tokenizer(num_words=None, lower=True, split=' ', char_level=False, oov_token='UNK')\n",
    "word_tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T22:00:40.200770Z",
     "start_time": "2019-12-10T22:00:40.196780Z"
    }
   },
   "outputs": [],
   "source": [
    "word_vocab_size = len(word_tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T22:12:39.043808Z",
     "start_time": "2019-12-10T22:12:39.037792Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38347"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T22:03:26.226321Z",
     "start_time": "2019-12-10T22:03:24.599673Z"
    }
   },
   "outputs": [],
   "source": [
    "encoded_train = [one_hot(s, word_vocab_size) for s in X_train]\n",
    "encoded_test = [one_hot(s, word_vocab_size) for s in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T22:04:03.244626Z",
     "start_time": "2019-12-10T22:04:02.849653Z"
    }
   },
   "outputs": [],
   "source": [
    "encoded_train = sequence.pad_sequences(encoded_train, maxlen=max_seq_len)\n",
    "encoded_test = sequence.pad_sequences(encoded_test, maxlen=max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T22:37:01.929186Z",
     "start_time": "2019-12-10T22:37:01.438500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 64, 128)           5120000   \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 15)                1935      \n",
      "=================================================================\n",
      "Total params: 5,220,751\n",
      "Trainable params: 5,220,751\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_model = Sequential()\n",
    "rnn_model.add(Embedding(40000, 128, input_length=max_seq_len))\n",
    "rnn_model.add(Bidirectional(LSTM(64)))\n",
    "rnn_model.add(Dropout(0.5))\n",
    "rnn_model.add(Dense(num_classes, activation='softmax'))\n",
    "rnn_model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "rnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T22:46:00.465155Z",
     "start_time": "2019-12-10T22:37:03.749318Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\zhuda\\anaconda3\\envs\\python3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 62356 samples, validate on 15590 samples\n",
      "Epoch 1/5\n",
      "62356/62356 [==============================] - 104s 2ms/step - loss: 0.2628 - accuracy: 0.9019 - val_loss: 0.2153 - val_accuracy: 0.9207\n",
      "Epoch 2/5\n",
      "62356/62356 [==============================] - 98s 2ms/step - loss: 0.2107 - accuracy: 0.9192 - val_loss: 0.2066 - val_accuracy: 0.9197\n",
      "Epoch 3/5\n",
      "62356/62356 [==============================] - 98s 2ms/step - loss: 0.1980 - accuracy: 0.9205 - val_loss: 0.2063 - val_accuracy: 0.9205\n",
      "Epoch 4/5\n",
      "62356/62356 [==============================] - 100s 2ms/step - loss: 0.1896 - accuracy: 0.9214 - val_loss: 0.2055 - val_accuracy: 0.9195\n",
      "Epoch 5/5\n",
      "62356/62356 [==============================] - 134s 2ms/step - loss: 0.1831 - accuracy: 0.9217 - val_loss: 0.2064 - val_accuracy: 0.9194\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x25178681160>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model.fit(encoded_train, text_y_train, \n",
    "              validation_data=(encoded_test, text_y_test),\n",
    "              batch_size=128,\n",
    "              verbose=1,\n",
    "              epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Char-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T22:31:34.617170Z",
     "start_time": "2019-12-10T22:31:34.613180Z"
    }
   },
   "outputs": [],
   "source": [
    "max_char_len = 256    # since tweets are not very long, so that I select a small number for the max character length for each seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T22:31:37.626152Z",
     "start_time": "2019-12-10T22:31:37.583234Z"
    }
   },
   "outputs": [],
   "source": [
    "train_texts = [s.lower() for s in X_train.values]\n",
    "test_texts = [s.lower() for s in X_test.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T22:31:43.788661Z",
     "start_time": "2019-12-10T22:31:39.098182Z"
    }
   },
   "outputs": [],
   "source": [
    "char_tokenizer = Tokenizer(num_words=None, char_level=True, oov_token='UNK')\n",
    "char_tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "alphabet = \"abcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
    "char_dict = {}\n",
    "for i, char in enumerate(alphabet):\n",
    "    char_dict[char] = i + 1\n",
    "    \n",
    "char_tokenizer.word_index = char_dict.copy()\n",
    "# add oov\n",
    "char_tokenizer.word_index[char_tokenizer.oov_token] = max(char_dict.values()) + 1\n",
    "\n",
    "train_seq = char_tokenizer.texts_to_sequences(train_texts)\n",
    "test_seq = char_tokenizer.texts_to_sequences(test_texts)\n",
    "\n",
    "train_seq = sequence.pad_sequences(train_seq, maxlen=max_char_len)\n",
    "test_seq = sequence.pad_sequences(test_seq, maxlen=max_char_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T22:31:43.797611Z",
     "start_time": "2019-12-10T22:31:43.791644Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_size = len(char_tokenizer.word_index)\n",
    "# conv, FFNN and other hyperparameters from https://arxiv.org/pdf/1509.01626.pdf\n",
    "conv_layers = [[256, 7, 3], [256, 7, 3], [256, 3, -1], [256, 3, -1], [256, 3, -1], [256, 3, 3]]\n",
    "FFNN_layers = [1024, 1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T22:31:43.807584Z",
     "start_time": "2019-12-10T22:31:43.800603Z"
    }
   },
   "outputs": [],
   "source": [
    "embed_weights = []\n",
    "# padding\n",
    "embed_weights.append(np.zeros(vocab_size))\n",
    "\n",
    "for char, index in char_tokenizer.word_index.items():\n",
    "    char_embed = np.zeros(vocab_size)\n",
    "    char_embed[i-1] = 1\n",
    "    embed_weights.append(char_embed)\n",
    "embed_weights = np.array(embed_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T22:48:28.305578Z",
     "start_time": "2019-12-10T22:48:27.994410Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "embedding_6 (Embedding)      (None, 256, 69)           4830      \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 250, 256)          123904    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 250, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 83, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 77, 256)           459008    \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 77, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 25, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 23, 256)           196864    \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 23, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 21, 256)           196864    \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 19, 256)           196864    \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 19, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 17, 256)           196864    \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 17, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1024)              1311744   \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 15)                15375     \n",
      "=================================================================\n",
      "Total params: 3,751,917\n",
      "Trainable params: 3,751,917\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_layer = Embedding(vocab_size+1, 69, input_length=max_char_len, weights=[embed_weights])\n",
    "inputs = Input(shape=(max_char_len,), name='input', dtype='int64')\n",
    "\n",
    "x = embed_layer(inputs)\n",
    "for layer in conv_layers:\n",
    "    x = Conv1D(layer[0], layer[1])(x)\n",
    "    x = Activation('relu')(x)\n",
    "    if layer[2] != -1:\n",
    "        x = MaxPooling1D(layer[2])(x)\n",
    "x = Flatten()(x)\n",
    "for feed_forward in FFNN_layers:\n",
    "    x = Dense(feed_forward, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "char_cnn_model = Model(inputs=inputs, outputs=predictions)\n",
    "char_cnn_model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "char_cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T23:15:26.807736Z",
     "start_time": "2019-12-10T22:48:36.637288Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\zhuda\\anaconda3\\envs\\python3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 62356 samples, validate on 15590 samples\n",
      "Epoch 1/5\n",
      "62356/62356 [==============================] - 335s 5ms/step - loss: 0.3309 - accuracy: 0.8774 - val_loss: 0.3182 - val_accuracy: 0.8803\n",
      "Epoch 2/5\n",
      "62356/62356 [==============================] - 356s 6ms/step - loss: 0.3160 - accuracy: 0.8800 - val_loss: 0.3105 - val_accuracy: 0.8805\n",
      "Epoch 3/5\n",
      "62356/62356 [==============================] - 317s 5ms/step - loss: 0.2978 - accuracy: 0.8832 - val_loss: 0.2867 - val_accuracy: 0.8878\n",
      "Epoch 4/5\n",
      "62356/62356 [==============================] - 297s 5ms/step - loss: 0.2794 - accuracy: 0.8897 - val_loss: 0.2733 - val_accuracy: 0.8920\n",
      "Epoch 5/5\n",
      "62356/62356 [==============================] - 303s 5ms/step - loss: 0.2673 - accuracy: 0.8974 - val_loss: 0.2631 - val_accuracy: 0.9005\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2517aa66128>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_cnn_model.fit(train_seq, text_y_train,\n",
    "                  validation_data=(test_seq, text_y_test),\n",
    "                  batch_size=128,\n",
    "                  epochs=5,\n",
    "                  verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use F1 to evaluate the model performance. The metric reports the precision, recall, and f1 by classes. We use the macro F1 as the main evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T23:19:50.044185Z",
     "start_time": "2019-12-10T23:19:45.105424Z"
    }
   },
   "outputs": [],
   "source": [
    "rnn_pred = rnn_model.predict(encoded_test)\n",
    "rnn_pred = np.where(rnn_pred >= 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T23:20:14.325302Z",
     "start_time": "2019-12-10T23:20:14.132817Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96    204399\n",
      "           1       0.96      0.37      0.54     29451\n",
      "\n",
      "   micro avg       0.92      0.92      0.92    233850\n",
      "   macro avg       0.94      0.69      0.75    233850\n",
      "weighted avg       0.92      0.92      0.90    233850\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flattened_pred_rnn = rnn_pred.flatten()\n",
    "flattened_true = text_y_test.flatten()\n",
    "print(classification_report(flattened_true, flattened_pred_rnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T23:21:53.345566Z",
     "start_time": "2019-12-10T23:21:36.107674Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95    204399\n",
      "           1       0.96      0.22      0.36     29451\n",
      "\n",
      "   micro avg       0.90      0.90      0.90    233850\n",
      "   macro avg       0.93      0.61      0.65    233850\n",
      "weighted avg       0.91      0.90      0.87    233850\n",
      "\n"
     ]
    }
   ],
   "source": [
    "char_cnn_pred = char_cnn_model.predict(test_seq)\n",
    "char_cnn_pred = np.where(char_cnn_pred >= 0.5, 1, 0)\n",
    "flatten_pred_char_cnn = char_cnn_pred.flatten()\n",
    "print(classification_report(flattened_true, flatten_pred_char_cnn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both Bi-LSTM and Char CNN are trained 5 epochs. For the word-level model, only tf-idf is used to calculate embedding weights. The Bi-LSTM model achieves 0.75 on F1 and Character-based CNN gets 0.65 on F1. The results from the two metrics show that the Bi-LSTM model achieves the better performance. If we do not increase the model complexity (especially for the Bi-LSTM), both non-contextualized embeddings, e.g. GloVe, and contextualized language models, e.g. ElMo, can largely improve the score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
